{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **INSTALLING NECESSARY LIBRARIES**"
      ],
      "metadata": {
        "id": "94FTbZVc3Xdm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Note:**\n",
        "Run This Notebook With GPU For Better Performance Or Change The Code Structure Accordingly To Run With CPU."
      ],
      "metadata": {
        "id": "16maIPic6d1X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMP105-j0fjB"
      },
      "outputs": [],
      "source": [
        "!pip install -q wfdb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Jui5mU3V6JgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp \"path of the dataset in gdrive\" ."
      ],
      "metadata": {
        "id": "gJ-FSJON6JAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFp_IrE2YYlv"
      },
      "source": [
        "### **IMPORT LIBRARIES**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KfJs_BHGROX4"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torchvision as tv\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import wfdb\n",
        "import time\n",
        "import random\n",
        "from sklearn.preprocessing import minmax_scale\n",
        "import sys\n",
        "from torchvision import transforms\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4A0omzyYnTs"
      },
      "source": [
        "### **CHANNEL CREATION AND REPRODUCEABILITY**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VUrTkVvhRxX0"
      },
      "outputs": [],
      "source": [
        "seed_num = 49\n",
        "torch.manual_seed(seed_num)\n",
        "run_num = 7\n",
        "channel_1 = 'v6'\n",
        "channel_2 = 'vz'\n",
        "channel_3 = 'ii'\n",
        "print(seed_num, run_num, channel_1, channel_2, channel_3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHPmJdz-Y8EC"
      },
      "source": [
        "### **LOAD REAL DATA (PTBDB)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knUb1nE3qeHS"
      },
      "outputs": [],
      "source": [
        "# Loading The Real Data (PTBDB)\n",
        "with open('ptbdb_data/RECORDS') as fp:  \n",
        "    lines = fp.readlines()\n",
        "\n",
        "files_unhealthy, files_healthy = [], []\n",
        "\n",
        "for file in lines:\n",
        "    file_path = \"ptbdb_data/\" + file[:-1] + \".hea\"\n",
        "    \n",
        "    # Reading The Header To Determine The Class\n",
        "    if 'Myocardial infarction' in open(file_path).read():\n",
        "        files_unhealthy.append(file)\n",
        "        \n",
        "    if 'Healthy control' in open(file_path).read():\n",
        "        files_healthy.append(file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIDcOjBAXi4J"
      },
      "source": [
        "###**SHUFFLING DATA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SW7eAUE0hryz"
      },
      "outputs": [],
      "source": [
        "# Shuffling Data (Cross-Validation)\n",
        "np.random.seed(seed_num)\n",
        "np.random.shuffle(files_unhealthy)\n",
        "np.random.shuffle(files_healthy)\n",
        "\n",
        "healthy_train = files_healthy[:int(0.8*len(files_healthy))]\n",
        "healthy_val = files_healthy[int(0.8*len(files_healthy)):]\n",
        "unhealthy_train = files_unhealthy[:int(0.8*len(files_unhealthy))]\n",
        "unhealthy_val = files_unhealthy[int(0.8*len(files_unhealthy)):]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **LIST OF INTERSECTIONS**"
      ],
      "metadata": {
        "id": "BPj18t_A7DU3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FE_60Z6wT8sW"
      },
      "outputs": [],
      "source": [
        "def intersection(lst1, lst2): \n",
        "    return list(set(lst1) & set(lst2)) \n",
        "\n",
        "patient_ids_unhealthy_train = [element[:10] for element in unhealthy_train]\n",
        "patient_ids_unhealthy_val = [element[:10] for element in unhealthy_val]\n",
        "patient_ids_healthy_train = [element[:10] for element in healthy_train]\n",
        "patient_ids_healthy_val = [element[:10] for element in healthy_val]\n",
        "\n",
        "intersection_unhealthy = intersection(patient_ids_unhealthy_train, patient_ids_unhealthy_val)\n",
        "intersection_healthy = intersection(patient_ids_healthy_train, patient_ids_healthy_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **INTERSECTION (UNHEALTHY)**"
      ],
      "metadata": {
        "id": "IsXObTJd7IWC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egFma8amUDpF"
      },
      "outputs": [],
      "source": [
        "# UnHealthy\n",
        "move_to_train = intersection_unhealthy[:int(0.5*len(intersection_unhealthy))]\n",
        "move_to_val = intersection_unhealthy[int(0.5*len(intersection_unhealthy)):]\n",
        "\n",
        "for patient_id in move_to_train:\n",
        "  in_val = []\n",
        "    \n",
        "  # Finding And Removing All Files In Val\n",
        "  for file_ in unhealthy_val:\n",
        "    if file_[:10] == patient_id:\n",
        "      in_val.append(file_)\n",
        "      unhealthy_val.remove(file_)\n",
        "            \n",
        "    # Adding To Train\n",
        "  for file_ in in_val:\n",
        "    unhealthy_train.append(file_)\n",
        "       \n",
        "    \n",
        "for patient_id in move_to_val:\n",
        "  in_train = []\n",
        "    \n",
        "    # Finding And Removing All Files In Val\n",
        "  for file_ in unhealthy_train:\n",
        "    if file_[:10] == patient_id:\n",
        "      in_train.append(file_)\n",
        "      unhealthy_train.remove(file_)\n",
        "            \n",
        "    # Adding To Train\n",
        "  for file_ in in_train:\n",
        "    unhealthy_val.append(file_)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **INTERSECTION (HEALTHY)**"
      ],
      "metadata": {
        "id": "V4XMBqZa7NBm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fS0ONBX0U5bs"
      },
      "outputs": [],
      "source": [
        "# Healthy\n",
        "move_to_train = intersection_healthy[:int(0.5*len(intersection_healthy))]\n",
        "move_to_val = intersection_healthy[int(0.5*len(intersection_healthy)):]\n",
        "\n",
        "for patient_id in move_to_train:\n",
        "  in_val = []\n",
        "    \n",
        "  # Finding And Removing All Files In Val\n",
        "  for file_ in healthy_val:\n",
        "    if file_[:10] == patient_id:\n",
        "      in_val.append(file_)\n",
        "      healthy_val.remove(file_)\n",
        "            \n",
        "  # Adding To Train\n",
        "  for file_ in in_val:\n",
        "    healthy_train.append(file_)\n",
        "        \n",
        "\n",
        "for patient_id in move_to_val:\n",
        "  in_train = []\n",
        "    \n",
        "  # Finding And Removing All Files In Val\n",
        "  for file_ in healthy_train:\n",
        "    if file_[:10] == patient_id:\n",
        "      in_train.append(file_)\n",
        "      healthy_train.remove(file_)\n",
        "            \n",
        "  # Adding To Train\n",
        "  for file_ in in_train:\n",
        "    healthy_val.append(file_)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **DATA SEPARATION FOR TRAINING AND VALIDATION**"
      ],
      "metadata": {
        "id": "xLt0Lng27QC8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TSqLJFDf5qm-"
      },
      "outputs": [],
      "source": [
        "data_healthy_train = []\n",
        "for file in healthy_train:\n",
        "  data_v4, _ = wfdb.rdsamp(\"ptbdb_data/\" + file[:-1], channel_names=[str(channel_1)])\n",
        "  data_v5, _ = wfdb.rdsamp(\"ptbdb_data/\" + file[:-1], channel_names=[str(channel_2)])\n",
        "  data_v6, _ = wfdb.rdsamp(\"ptbdb_data/\" + file[:-1], channel_names=[str(channel_3)])\n",
        "  data = [data_v4.flatten(), data_v5.flatten(), data_v6.flatten()]\n",
        "  data_healthy_train.append(data)\n",
        "data_healthy_val = []\n",
        "for file in healthy_val:\n",
        "  data_v4, _ = wfdb.rdsamp(\"ptbdb_data/\" + file[:-1], channel_names=[str(channel_1)])\n",
        "  data_v5, _ = wfdb.rdsamp(\"ptbdb_data/\" + file[:-1], channel_names=[str(channel_2)])\n",
        "  data_v6, _ = wfdb.rdsamp(\"ptbdb_data/\" + file[:-1], channel_names=[str(channel_3)])\n",
        "  data = [data_v4.flatten(), data_v5.flatten(), data_v6.flatten()]\n",
        "  data_healthy_val.append(data)\n",
        "data_unhealthy_train = []\n",
        "for file in unhealthy_train:\n",
        "  data_v4, _ = wfdb.rdsamp(\"ptbdb_data/\" + file[:-1], channel_names=[str(channel_1)])\n",
        "  data_v5, _ = wfdb.rdsamp(\"ptbdb_data/\" + file[:-1], channel_names=[str(channel_2)])\n",
        "  data_v6, _ = wfdb.rdsamp(\"ptbdb_data/\" + file[:-1], channel_names=[str(channel_3)])\n",
        "  data = [data_v4.flatten(), data_v5.flatten(), data_v6.flatten()]\n",
        "  data_unhealthy_train.append(data)\n",
        "data_unhealthy_val = []\n",
        "for file in unhealthy_val:\n",
        "  data_v4, _ = wfdb.rdsamp(\"ptbdb_data/\" + file[:-1], channel_names=[str(channel_1)])\n",
        "  data_v5, _ = wfdb.rdsamp(\"ptbdb_data/\" + file[:-1], channel_names=[str(channel_2)])\n",
        "  data_v6, _ = wfdb.rdsamp(\"ptbdb_data/\" + file[:-1], channel_names=[str(channel_3)])\n",
        "  data = [data_v4.flatten(), data_v5.flatten(), data_v6.flatten()]\n",
        "  data_unhealthy_val.append(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjizOEiXU_2s"
      },
      "outputs": [],
      "source": [
        "data_healthy_train = np.asarray(data_healthy_train, object)\n",
        "data_healthy_val = np.asarray(data_healthy_val, object)\n",
        "data_unhealthy_train = np.asarray(data_unhealthy_train, object)\n",
        "data_unhealthy_val = np.asarray(data_unhealthy_val, object)\n",
        "\n",
        "window_size = 10000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kwfk63K60hdw"
      },
      "outputs": [],
      "source": [
        "#Training Sets\n",
        "data_unhealthy_train_np = np.arange(len(data_unhealthy_train))\n",
        "data_healthy_train_np = np.arange(len(data_healthy_train))\n",
        "\n",
        "#Validation Sets\n",
        "data_unhealthy_val_np = np.arange(len(data_unhealthy_val))\n",
        "data_healthy_val_np = np.arange(len(data_healthy_val))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hv-RM1oxZ-eF"
      },
      "source": [
        "### **DEFINING 'GET BATCHES' FUNCTION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RpRcFv4Tt1U-"
      },
      "outputs": [],
      "source": [
        "def get_batch(batch_size, split='train'):\n",
        "  if split == 'train':\n",
        "    unhealthy_indices = random.sample(tuple(data_unhealthy_train_np), k=int(batch_size / 2))\n",
        "    healthy_indices = random.sample(tuple(data_healthy_train_np), k=int(batch_size / 2))\n",
        "    unhealthy_batch = data_unhealthy_train[unhealthy_indices]\n",
        "    healthy_batch = data_healthy_train[healthy_indices]\n",
        "  elif split == 'val': \n",
        "    unhealthy_indices = random.sample(tuple(data_unhealthy_val_np), k=int(batch_size / 2))\n",
        "    healthy_indices = random.sample(tuple(data_healthy_val_np), k=int(batch_size / 2))\n",
        "    unhealthy_batch = data_unhealthy_val[unhealthy_indices]\n",
        "    healthy_batch = data_healthy_val[healthy_indices]\n",
        "    \n",
        "  batch_x = []\n",
        "  for sample in unhealthy_batch:\n",
        "    \n",
        "    start = random.choice(np.arange(len(sample[0]) - window_size))\n",
        "\n",
        "    # Normalize\n",
        "    normalized_1 = minmax_scale(sample[0][start:start+window_size])\n",
        "    normalized_2 = minmax_scale(sample[1][start:start+window_size])\n",
        "    normalized_3 = minmax_scale(sample[2][start:start+window_size])\n",
        "    normalized = np.array((normalized_1, normalized_2, normalized_3))\n",
        "        \n",
        "    batch_x.append(normalized)\n",
        "        \n",
        "  for sample in healthy_batch:\n",
        "    start = random.choice(np.arange(len(sample[0]) - window_size))\n",
        "        \n",
        "    # Normalize\n",
        "    normalized_1 = minmax_scale(sample[0][start:start+window_size])\n",
        "    normalized_2 = minmax_scale(sample[1][start:start+window_size])\n",
        "    normalized_3 = minmax_scale(sample[2][start:start+window_size])\n",
        "    normalized = np.array((normalized_1, normalized_2, normalized_3))\n",
        "        \n",
        "    batch_x.append(normalized)\n",
        "    \n",
        "  batch_y = [0.1 for _ in range(int(batch_size / 2))]\n",
        "  for _ in range(int(batch_size / 2)):\n",
        "    batch_y.append(0.9)\n",
        "        \n",
        "  indices = np.arange(len(batch_y))\n",
        "  np.random.shuffle(indices)\n",
        "    \n",
        "  batch_x = np.array(batch_x)\n",
        "  batch_y = np.array(batch_y)\n",
        "    \n",
        "  batch_x = batch_x[indices]\n",
        "  batch_y = batch_y[indices]\n",
        "    \n",
        "  batch_x = np.reshape(batch_x, (-1, 3, window_size))\n",
        "  batch_x = torch.from_numpy(batch_x)\n",
        "  batch_x = batch_x.float().cuda()\n",
        "  batch_x = batch_x.float()\n",
        "    \n",
        "  batch_y = np.reshape(batch_y, (-1, 1))\n",
        "  batch_y = torch.from_numpy(batch_y)\n",
        "  batch_y = batch_y.float().cuda()\n",
        "  batch_y = batch_y.float()\n",
        "  \n",
        "  return batch_x, batch_y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYl4-K9ar84P"
      },
      "source": [
        "### **CNN ARCHITECTURE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5nGkAYnr3kG"
      },
      "outputs": [],
      "source": [
        "class ConvNetQuake(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(ConvNetQuake, self).__init__()\n",
        "    \n",
        "    self.conv1 = nn.Conv1d(in_channels=3, out_channels=32, kernel_size=3, stride=2, padding=1)\n",
        "    self.conv2 = nn.Conv1d(in_channels=32, out_channels=32, kernel_size=3, stride=2, padding=1)\n",
        "    self.conv3 = nn.Conv1d(in_channels=32, out_channels=32, kernel_size=3, stride=2, padding=1)\n",
        "    self.conv4 = nn.Conv1d(in_channels=32, out_channels=32, kernel_size=3, stride=2, padding=1)\n",
        "    self.conv5 = nn.Conv1d(in_channels=32, out_channels=32, kernel_size=3, stride=2, padding=1)\n",
        "    self.conv6 = nn.Conv1d(in_channels=32, out_channels=32, kernel_size=3, stride=2, padding=1)\n",
        "    self.conv7 = nn.Conv1d(in_channels=32, out_channels=32, kernel_size=3, stride=2, padding=1)\n",
        "    self.conv8 = nn.Conv1d(in_channels=32, out_channels=32, kernel_size=3, stride=2, padding=1)\n",
        "    self.linear1 = nn.Linear(1280, 128)\n",
        "    self.linear2 = nn.Linear(128, 1)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "    self.bn1 = nn.BatchNorm1d(32)\n",
        "    self.bn2 = nn.BatchNorm1d(32)\n",
        "    self.bn3 = nn.BatchNorm1d(32)\n",
        "    self.bn4 = nn.BatchNorm1d(32)\n",
        "    self.bn5 = nn.BatchNorm1d(32)\n",
        "    self.bn6 = nn.BatchNorm1d(32)\n",
        "    self.bn7 = nn.BatchNorm1d(32)\n",
        "    self.bn8 = nn.BatchNorm1d(32)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.bn1(F.relu((self.conv1(x))))\n",
        "    x = self.bn2(F.relu((self.conv2(x))))\n",
        "    x = self.bn3(F.relu((self.conv3(x))))\n",
        "    x = self.bn4(F.relu((self.conv4(x))))\n",
        "    x = self.bn5(F.relu((self.conv5(x))))\n",
        "    x = self.bn6(F.relu((self.conv6(x))))\n",
        "    x = self.bn7(F.relu((self.conv7(x))))\n",
        "    x = self.bn8(F.relu((self.conv8(x))))\n",
        "    x = x.view(x.size(0), -1) \n",
        "    x = self.linear1(x)\n",
        "    x = self.linear2(x)\n",
        "    x = self.sigmoid(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSlaufBXsFoC"
      },
      "source": [
        "### **DEFINING MODEL AND PARAMETERS**\n",
        "1. MODEL = CONVNETQUAKE\n",
        "2. OPTIMISER = ADAM\n",
        "3. REGULARIZER = L2 REGULARIZER(RIDGE REGULARIZER)\n",
        "4. LOSS FUNCTION = BINARY CROSS-ENTROPY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "epYH7TY8sFR5"
      },
      "outputs": [],
      "source": [
        "# Defining The Model\n",
        "model = ConvNetQuake()\n",
        "model.cuda()\n",
        "\n",
        "model = nn.DataParallel(model, device_ids=[0])\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.0001)\n",
        "criterion = nn.BCELoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClRl8x3S7bO9"
      },
      "source": [
        "### **TRAINING AND VALIDATION OF THE MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehHmWNMMpmME"
      },
      "outputs": [],
      "source": [
        "# Training loop\n",
        "writer = SummaryWriter('/content/Runs')\n",
        "\n",
        "# num_iters = 30000\n",
        "# num_iters = 35000\n",
        "num_iters = 5000\n",
        "batch_size = 10\n",
        "\n",
        "acc_values = []\n",
        "acc_values_train = []\n",
        "\n",
        "for iters in range(num_iters):\n",
        "  batch_x, batch_y = get_batch(batch_size, split='train')\n",
        "  y_pred = model(batch_x)\n",
        "  loss = criterion(y_pred, batch_y)\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        " \n",
        "  # Validation\n",
        "  if iters%100 == 0 and iters != 0:\n",
        "    writer.add_scalar('Loss/train', loss, iters)\n",
        "    with torch.no_grad():\n",
        "      # Test_set\n",
        "      iterations = 100\n",
        "      avg_acc = 0\n",
        "      for _ in range(iterations):\n",
        "        batch_x, batch_y = get_batch(batch_size, split='val')\n",
        "        cleaned = model(batch_x)\n",
        "        count = 0\n",
        "        acc = 0\n",
        "        for num in cleaned:\n",
        "          if int(torch.round(num)) == int(torch.round(batch_y[count])):\n",
        "            acc += 10\n",
        "            count += 1\n",
        "        avg_acc += acc\n",
        "        \n",
        "      acc_values.append((avg_acc / iterations))\n",
        "      writer.add_scalar('Accuracy/val', (avg_acc / iterations), iters)\n",
        "      \n",
        "      # Train_set\n",
        "      iterations = 100\n",
        "      avg_acc_train = 0\n",
        "      \n",
        "      for _ in range(iterations):\n",
        "        batch_x, batch_y = get_batch(batch_size, split='train')\n",
        "        cleaned = model(batch_x)\n",
        "        \n",
        "        count = 0\n",
        "        acc = 0\n",
        "        for num in cleaned:\n",
        "          if int(torch.round(num)) == int(torch.round(batch_y[count])):\n",
        "            acc += 10\n",
        "            count += 1\n",
        "        avg_acc_train += acc\n",
        "        \n",
        "      acc_values_train.append((avg_acc_train / iterations))\n",
        "      writer.add_scalar('Accuracy/train', (avg_acc_train / iterations), iters)\n",
        "        \n",
        "        # Printing The Values Of Iters, Loss, And Accuracy\n",
        "    print(f\"Iteration {iters}: Loss = {loss:.4f}, Validation Accuracy = {(avg_acc / iterations):.4f}, Train Accuracy = {(avg_acc_train / iterations):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **LOADING THE TRAINED MODEL AND PREDICTING CUSTOM ECG SIGNALS**"
      ],
      "metadata": {
        "id": "q5-iFbNiJoTu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = nn.DataParallel(model)\n",
        "model.load_state_dict(torch.load(\"MID_Model_Dicts.pth\", map_location=torch.device('cpu')))"
      ],
      "metadata": {
        "id": "TJQxHWWyPeS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(input_path:str,model):\n",
        "  record = input_path\n",
        "\n",
        "  #Read The Input ECG Signals And Convert Into NP Array\n",
        "  data, _ = wfdb.rdsamp(record)\n",
        "  data_np = np.asarray(data)\n",
        "  start = random.choice(np.arange(len(data_np) - 10000))\n",
        "  \n",
        "  #Normalize The Array With MinMax Scale\n",
        "  normalized = minmax_scale(data_np[start:start + 10000])\n",
        "  \n",
        "  #Reshape According To The Trained Model Size\n",
        "  np_reshape = np.reshape(normalized, (-1, 3, 10000))\n",
        "  \n",
        "  #Convert Arrays To Tensors\n",
        "  pred_torch = torch.from_numpy(np_reshape)\n",
        "  pred_float = pred_torch.float()\n",
        "  \n",
        "  #Evaluate Using The Model\n",
        "  with torch.no_grad():\n",
        "    model.eval()\n",
        "    ecg_pred = model(pred_float)\n",
        "    ecg_pred_probs = torch.sigmoid(ecg_pred)*100\n",
        "    pred = torch.mean(ecg_pred_probs)\n",
        "    prediction = round(float(pred.item()))\n",
        "\n",
        "  #Limit The Predictions Accordingly\n",
        "    if prediction <= 50:\n",
        "      print(f\"This Patient Has Myocardial Infarction\")\n",
        "    else:\n",
        "      print(f\"This Patient Is Healthy\")"
      ],
      "metadata": {
        "id": "1PNVp-G2KjHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "record_path = \"/content/s0010_re\""
      ],
      "metadata": {
        "id": "6KqUmKTk8d4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict(record_path, model = model)"
      ],
      "metadata": {
        "id": "K1-1Z43yL9Ei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **PLOTTING THE RESPECTIVE ECG SIGNAL**"
      ],
      "metadata": {
        "id": "X60j9Xo_4PCo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading The ECG Signal\n",
        "record_ecg = wfdb.rdrecord(record_path, sampfrom=0, sampto=1000, channels=[0,1])\n",
        "signal = record_ecg.p_signal.flatten()\n",
        "\n",
        "# Create A Time Array Based On The Sampling Frequency\n",
        "fs = record_ecg.fs\n",
        "time = [i / fs for i in range(len(signal))]\n",
        "\n",
        "# Plot The ECG Signal\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.plot(time, signal)\n",
        "plt.xlabel('Time (s)')\n",
        "plt.ylabel('Amplitude')\n",
        "plt.title('ECG Signal')\n",
        "plt.show()\n",
        "plt.savefig('ecg_signal.png')"
      ],
      "metadata": {
        "id": "Fwhcdn0r3LDG"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}